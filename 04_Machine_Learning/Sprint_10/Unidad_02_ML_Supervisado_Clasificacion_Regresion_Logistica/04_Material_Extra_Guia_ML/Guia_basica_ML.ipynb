{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPXWUFKZ5D7r"
      },
      "source": [
        "## Paso 0: Importar librerías"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFReNc0-4c1u"
      },
      "source": [
        "Antes de empezar, cargamos las librerías necesarias para manejar datos, hacer gráficos, preparar nuestros datos y entrenar modelos. Esto nos permite tener todas las herramientas listas desde el principio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qm_Lom-s7kDo"
      },
      "outputs": [],
      "source": [
        "# Manipulación de datos\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualización\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Modelado y preprocesamiento\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Modelos de ejemplo\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "\n",
        "# Métricas\n",
        "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error, r2_score\n",
        "\n",
        "# Otros\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')  # evita mensajes molestos en el notebook\n",
        "\n",
        "pd.options.mode.copy_on_write = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3M0H7c85HOe"
      },
      "source": [
        "## Paso 1: Cargar datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LumynC7N4jDm"
      },
      "source": [
        "Primero necesitamos traer nuestros datos al entorno de trabajo. Usualmente vienen en archivos CSV, Excel o desde una base de datos. En esta etapa solo leemos los datos y echamos un primer vistazo rápido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEQWaRUx75q-"
      },
      "outputs": [],
      "source": [
        "# Cargar un CSV\n",
        "df = pd.read_csv(\"data/ejemplo_housing.csv\")\n",
        "\n",
        "# Primeras filas para ver cómo es el dataset\n",
        "df.head()\n",
        "\n",
        "# Información básica del dataset\n",
        "df.info()\n",
        "\n",
        "# Estadísticas básicas de las columnas numéricas\n",
        "df.describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF1p8MqX5l3J"
      },
      "source": [
        "## Paso 2: Definición del problema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xV7jpL434lKf"
      },
      "source": [
        "Antes de tocar los datos, debemos saber qué queremos resolver:\n",
        "\n",
        "¿Queremos predecir un valor numérico? → Regresión\n",
        "\n",
        "¿Queremos clasificar en categorías? → Clasificación\n",
        "\n",
        "Esto nos ayuda a decidir qué modelos y métricas usar.\n",
        "\n",
        "Advertencia: A veces los targets son números pero representan categorías (por ejemplo, códigos de producto o clases), en ese caso es clasificación, no regresión."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxtFLN9A5tr6"
      },
      "source": [
        "## Paso 3: Divisón de Train y Test\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLW5rD3w40lM"
      },
      "source": [
        "Antes de empezar a explorar o modelar los datos, siempre separamos una parte para probar nuestro modelo después:\n",
        "\n",
        "Train → para entrenar el modelo\n",
        "\n",
        "Test → para evaluar si el modelo generaliza bien\n",
        "\n",
        "\n",
        "\n",
        "Se suele usar 70-80% para train y 20-30% para test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e260XIYF5_i1"
      },
      "source": [
        "## Paso 4: Limpieza de los datos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9xTONF79w7P"
      },
      "source": [
        "Antes de explorar los datos, conviene hacer una limpieza básica: revisar valores faltantes graves, asegurarse de que los tipos son correctos y tratar codificaciones simples.\n",
        "\n",
        "La estandarización y transformaciones más avanzadas se harán después, en el tratamiento de variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUjL_NsH_xid"
      },
      "outputs": [],
      "source": [
        "# Revisar valores faltantes\n",
        "print(df.isna().sum())\n",
        "\n",
        "# Rellenar NaN simples (ejemplo con media)\n",
        "df['edad'] = df['edad'].fillna(df['edad'].mean())\n",
        "\n",
        "# Eliminar filas duplicadas si es necesario\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# Revisar tipos de datos\n",
        "print(df.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a98__rmb6Td7"
      },
      "source": [
        "## Paso 5: Comprensión de variables, mini-EDA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNYKXUrnD6xa"
      },
      "source": [
        "### Pairplot inicial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VdyQD1mD44T"
      },
      "source": [
        "Visualización rápida de varias variables a la vez. Útil para detectar patrones, clusters o interacciones importantes, especialmente cuando no hay demasiadas features continuas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQmN3vTDB2W-"
      },
      "source": [
        "### Estudio de la target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo53q1pCB4oS"
      },
      "source": [
        "Antes de mirar cualquier feature, observa tu variable objetivo. Comprueba su distribución y si está desbalanceada: esto influye en la elección de métricas y modelos. Para clasificación, revisa cuántas muestras hay de cada clase; para regresión, analiza la dispersión y posibles outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuQ5NmTsB-d7"
      },
      "outputs": [],
      "source": [
        "# Clasificación\n",
        "sns.countplot(x='target', data=df)\n",
        "\n",
        "# Regresión\n",
        "sns.histplot(df['target'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTcr4_1ACDt7"
      },
      "source": [
        "### Análisis de correlaciones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY7f8BHoCF_h"
      },
      "source": [
        "Revisa cómo se relacionan tus features entre sí y con el target. Correlaciones muy altas pueden indicar redundancia; correlaciones con el target ayudan a ver qué features podrían ser más relevantes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqpLynNOCGnz"
      },
      "outputs": [],
      "source": [
        "# Matriz de correlaciones\n",
        "sns.heatmap(df.corr(numeric_only = True), annot=True, vmin = -1, vmax = 1, cmap='coolwarm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNuYCKJXCaP4"
      },
      "source": [
        "Ponemos los parámetros de vmin y vmax en -1 y 1 para ver la distribución estandarizada de las correlaciones.\n",
        "\n",
        "Cuanto más cerca del 1, será correlación positiva; cuanto más cerca del -1, correlación negativa.\n",
        "\n",
        "Cuanto más cerca del 0, más irrelevante.\n",
        "\n",
        "El parámetro de cmap='coolwarm' nos ayuda a analizar esto a golpe de vista."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KHgJruzDRyQ"
      },
      "source": [
        "#### Advertencia sobre multicolinealidad:\n",
        "Si dos o más features están muy correlacionadas entre sí, decimos que existe multicolinealidad. Esto puede afectar negativamente a algunos modelos (como regresión lineal) porque dificulta que el modelo separe el efecto de cada variable. En ese caso, conviene eliminar o combinar features muy correlacionadas antes de entrenar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEwJFeKqDcp0"
      },
      "source": [
        "### Análisis univariante de features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1j-nU15LDgaK"
      },
      "source": [
        "Observa cada feature individualmente: distribución, valores extremos, posibles outliers. Esto te ayuda a detectar problemas antes de modelar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-nLt-BVDgM5"
      },
      "outputs": [],
      "source": [
        "# Ejemplo: Histograma para todas las features numéricas\n",
        "df.hist(figsize=(12,8), bins=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZX8nCuADrDq"
      },
      "source": [
        "### Análisis bivariante\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBc7bNpZDqd6"
      },
      "source": [
        "Compara cada feature con el target o entre sí para ver relaciones. Para regresión, scatterplots; para clasificación, boxplots o violinplots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cmt3NkdJDvsj"
      },
      "outputs": [],
      "source": [
        "# Scatterplot para regresión\n",
        "sns.scatterplot(x='feature1', y='target', data=df)\n",
        "\n",
        "# Boxplot para clasificación\n",
        "sns.boxplot(x='target', y='feature2', data=df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmgID25K6Wg6"
      },
      "source": [
        "## Paso 6: Tratamiento de variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jawJOY3B7g5I"
      },
      "source": [
        "En este paso preparamos los datos para que puedan ser usados por modelos de Machine Learning. Incluye la eliminación de información poco útil, el tratamiento de problemas comunes en los datos y la creación o transformación de variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83Ed-pV57lue"
      },
      "source": [
        "### 6.1 Eliminación de features\n",
        "Tras el EDA, evaluamos si sobran variables. No todas las features aportan información predictiva y algunas pueden incluso perjudicar al modelo.\n",
        "Suelen eliminarse:\n",
        "Features con valor constante.\n",
        "Features con muchos missings (≈ >20–30%).\n",
        "Identificadores (IDs, nombres propios).\n",
        "Strings largos sin tratamiento NLP.\n",
        "Variables con alta cardinalidad.\n",
        "Features muy correlacionadas entre sí (multicolinealidad).\n",
        "Estas decisiones dependen del número total de features: cuantos más datos tengamos, más margen hay para eliminar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9beDtHY7r03"
      },
      "outputs": [],
      "source": [
        "# Eliminar columnas manualmente\n",
        "df.drop(columns=['feature1', 'feature2'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YKlgezl7sWs"
      },
      "source": [
        "### 6.2 Duplicados\n",
        "Los duplicados no suelen aportar información y normalmente se eliminan.\n",
        "Antes, es importante identificar qué define una fila única (cliente, pedido, cliente + fecha, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3ITmwhL7wGo"
      },
      "outputs": [],
      "source": [
        "# Eliminar duplicados completos\n",
        "df.drop_duplicates()\n",
        "\n",
        "# Eliminar duplicados según una o varias columnas\n",
        "df.drop_duplicates(subset=['cliente'], keep='last')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsVHuZcG7v3J"
      },
      "source": [
        "### 6.3 Missings (valores faltantes)\n",
        "Un missing es un valor ausente (NaN), no un 0 ni un string vacío.\n",
        "\n",
        "La mayoría de modelos no los admiten, así que deben tratarse.\n",
        "\n",
        "Opciones principales:\n",
        "\n",
        "Eliminar filas o columnas (si son pocos).\n",
        "\n",
        "- Imputar con media, mediana o moda.\n",
        "\n",
        "- Imputar con un valor concreto.\n",
        "\n",
        "- Imputación avanzada (KNN, modelos).\n",
        "\n",
        "- Imputación + flag indicando que había missing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjuR8WOZ8Bcr"
      },
      "outputs": [],
      "source": [
        "# Eliminar filas con missings\n",
        "df.dropna()\n",
        "\n",
        "# Imputar con la media\n",
        "df['feature'] = df['feature'].fillna(df['feature'].mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOeTN4GE8EMd"
      },
      "source": [
        "### 6.4 Anomalías y errores\n",
        "No son outliers, sino datos incorrectos:\n",
        "\n",
        "- Valores negativos imposibles.\n",
        "- Fechas mal leídas (formato, año 9999).\n",
        "- Problemas de encoding en texto.\n",
        "\n",
        "Suelen tratarse como missings o corregirse manualmente tras revisión."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRYsGKUM8Lof"
      },
      "source": [
        "### 6.5 Outliers\n",
        "Valores atípicos que se alejan mucho del resto.\n",
        "\n",
        "Pueden penalizar modelos basados en distancias o gradiente.\n",
        "\n",
        "Detección:\n",
        "- Gráficos (boxplot, histograma, scatter).\n",
        "- Media ± N·desviación estándar.\n",
        "\n",
        "Tratamiento:\n",
        "- Eliminarlos.\n",
        "- No hacer nada (árboles y SVM son robustos).\n",
        "- Transformaciones (log).\n",
        "- Binning.\n",
        "- Imputación.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qX0Gtp9b8X0c"
      },
      "source": [
        "⚠️ Si eliminas outliers en X_train, elimina las mismas filas en y_train."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJvg50_s8YjU"
      },
      "source": [
        "### 6.7 Transformaciones\n",
        "Buscan mejorar la forma de la distribución (asimetría).\n",
        "\n",
        "Logarítmica (la más común).\n",
        "\n",
        "Cuadrada / cúbica.\n",
        "\n",
        "Box-Cox.\n",
        "\n",
        "Se evalúa con histogramas, skew o test de Shapiro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz0X9NNO8gBM"
      },
      "source": [
        "### 6.8 Encodings (variables categóricas)\n",
        "\n",
        "Los modelos no admiten texto, hay que convertirlo a números.\n",
        "\n",
        "Elección según el tipo de variable:\n",
        "- Binaria → mapeo 0/1.\n",
        "- Ordenada → mapeo respetando el orden.\n",
        "- No ordenada → OneHot / dummies.\n",
        "- Alta cardinalidad → Hashing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-C0fYcKY8l0l"
      },
      "outputs": [],
      "source": [
        "# One-hot encoding\n",
        "pd.get_dummies(df, columns=['city'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwLTbYe18oKi"
      },
      "source": [
        "⚠️ Codificar siempre usando train, y aplicar lo mismo a test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCxFKB0L8qQZ"
      },
      "source": [
        "### 6.9 Escalado\n",
        "\n",
        "Pone todas las variables en la misma escala.\n",
        "\n",
        "- StandardScaler (media 0, std 1).\n",
        "\n",
        "- MinMaxScaler (rango 0–1).\n",
        "\n",
        "Importante para modelos basados en distancias o gradiente.\n",
        "No afecta a árboles.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Etf-owBg8wF4"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6jCGvko6i4i"
      },
      "source": [
        "## Paso 7: Elección de métricas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n76XBZ1s891P"
      },
      "source": [
        "Ningún modelo es perfecto: siempre comete errores. Por eso necesitamos una métrica, que nos sirva para:\n",
        "- Comparar distintos modelos y elegir el mejor.\n",
        "- Comunicar resultados de forma clara y honesta.\n",
        "\n",
        "La métrica debe elegirse según el tipo de problema y el contexto, no solo porque sea la más conocida. La mayoría están implementadas en sklearn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rH4rvDN9G8E"
      },
      "source": [
        "### 7.1 Métricas para clasificación\n",
        "#### Accuracy\n",
        "- Porcentaje total de aciertos.\n",
        "- Fácil de entender.\n",
        "- Poco informativa si las clases están desbalanceadas.\n",
        "\n",
        "#### Matriz de confusión\n",
        "Muestra en qué se equivoca el modelo.\n",
        "- TP: positivos bien clasificados\n",
        "- TN: negativos bien clasificados\n",
        "- FP: falsos positivos\n",
        "- FN: falsos negativos\n",
        "\n",
        "Es la base para entender el resto de métricas.\n",
        "\n",
        "#### Recall (Sensibilidad)\n",
        "De todos los positivos reales, cuántos detecta el modelo.\n",
        "\n",
        "Importante cuando no queremos FN.\n",
        "\n",
        "Ejemplo típico: detección de enfermedades.\n",
        "\n",
        "#### Precision\n",
        "De todos los positivos predichos, cuántos son correctos.\n",
        "\n",
        "Importante cuando no queremos FP.\n",
        "\n",
        "Ejemplo: filtro de spam.\n",
        "\n",
        "#### Curvas Precision–Recall\n",
        "Permiten analizar el comportamiento del modelo cambiando el threshold.\n",
        "- Útiles cuando las clases están desbalanceadas.\n",
        "- Ayudan a elegir el punto óptimo según el problema.\n",
        "\n",
        "#### Curva ROC y AUC\n",
        "Relaciona Recall con la tasa de falsos positivos.\n",
        "- AUC mide el rendimiento global del clasificador.\n",
        "- AUC ≈ 1 → buen modelo\n",
        "- AUC ≈ 0.5 → modelo aleatorio\n",
        "Más usada en datasets balanceados.\n",
        "\n",
        "#### ¿Cómo saber qué metrica de clasificación usar?\n",
        "Imagina un modelo que detecta si un email es spam. Si solo miramos accuracy, puede parecer muy bueno porque acierta casi todo, pero quizá falla justo en los correos importantes. Recall sería clave si no queremos que se nos escape ningún spam (aunque marque algún correo bueno como spam), mientras que precision es más importante si nos preocupa no perder ningún correo importante. La matriz de confusión nos permite ver exactamente en qué se equivoca el modelo: qué correos confunde y cómo. Cuando no está claro si priorizar recall o precision, usamos el F1-score, que busca un equilibrio entre ambos. Si además queremos analizar cómo cambia el comportamiento del modelo al ser más o menos estricto, usamos curvas como ROC o precision–recall, que nos ayudan a elegir el punto óptimo según el coste real de cada tipo de error.\n",
        "\n",
        "### 7.2 Métricas para regresión\n",
        "#### R² (coeficiente de determinación)\n",
        "- Mide la proporción de variabilidad explicada por el modelo.\n",
        "- No indica por sí solo si el modelo es bueno.\n",
        "- Puede ser engañoso si se usa sin contexto.\n",
        "\n",
        "#### MSE (Mean Squared Error)\n",
        "- Media del error al cuadrado.\n",
        "- Penaliza mucho los errores grandes.\n",
        "- Útil para comparar modelos, no para explicar resultados.\n",
        "\n",
        "#### RMSE (Root MSE)\n",
        "- Raíz del MSE, en las unidades del target.\n",
        "- Más interpretable que MSE.\n",
        "- Muy usada para reportar resultados.\n",
        "\n",
        "#### MAE (Mean Absolute Error)\n",
        "- Error medio absoluto.\n",
        "- Fácil de interpretar.\n",
        "- Más robusta frente a outliers.\n",
        "\n",
        "#### MAPE (Mean Absolute Percentage Error)\n",
        "- Error medio en porcentaje.\n",
        "- Útil para comparar modelos con distintos targets.\n",
        "- Acotada entre 0 y 100.\n",
        "- Problemas si el target tiene valores cercanos a 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duGxBKQl65dD"
      },
      "source": [
        "## Paso 8: Selección del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aQsUkYV-4bU"
      },
      "source": [
        "No existe un modelo “mejor” en general: el modelo adecuado depende de los datos y del objetivo del problema. Antes de entrenar, conviene tener en cuenta varios factores clave.\n",
        "\n",
        "### Volumen de datos y número de features\n",
        "- Pocos datos y muchas variables: conviene usar modelos simples, con mayor sesgo pero que generalizan mejor (regresión lineal, Naive Bayes, SVM lineal).\n",
        "- Muchos datos y pocas variables: se pueden usar modelos más complejos, capaces de capturar patrones más finos (KNN, árboles de decisión, SVM con kernel).\n",
        "### Precisión vs interpretabilidad\n",
        "- Uno de los compromisos más importantes.\n",
        "- Modelos interpretables (“caja blanca”) permiten explicar por qué toman una decisión (regresiones, modelos lineales).\n",
        "- Modelos más complejos (“caja negra”) suelen ser más precisos, pero difíciles de explicar (ensembles, redes neuronales).\n",
        "- La elección depende de si necesitamos justificar las decisiones del modelo.\n",
        "### Velocidad de entrenamiento\n",
        "- Rápidos: regresión logística, modelos lineales, Naive Bayes.\n",
        "- Lentos: SVM (especialmente con tuning), ensembles, redes neuronales.\n",
        "- Esto es importante cuando hay muchos datos o poco tiempo de cómputo.\n",
        "### Tipo de relación entre los datos\n",
        "- Si la relación con el target es aproximadamente lineal, los modelos lineales suelen funcionar bien.\n",
        "- Si existen relaciones no lineales, conviene usar modelos más flexibles como árboles, random forest, KNN o redes neuronales.\n",
        "- Esto se detecta principalmente durante el EDA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "002eSCV_67nI"
      },
      "source": [
        "## Paso 9: Entrenamiento del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-P0OiP0_VEd"
      },
      "source": [
        "En este paso el modelo aprende patrones a partir de los datos de entrenamiento. El objetivo no es solo que funcione bien en train, sino que generalice correctamente a datos nuevos.\n",
        "\n",
        "Antes de entrenar, es importante:\n",
        "- Haber separado correctamente train y test.\n",
        "- Aplicar el preprocesado solo con train.\n",
        "- Definir claramente la métrica con la que se evaluará el modelo.\n",
        "\n",
        "Durante el entrenamiento:\n",
        "- Se entrena primero un modelo base (baseline) sin mucho ajuste.\n",
        "- Después, si es necesario, se prueban otros modelos o se ajustan hiperparámetros.\n",
        "- El uso de pipelines asegura que el preprocesado y el modelo se apliquen siempre de forma consistente y sin fugas de información.\n",
        "\n",
        "Tras entrenar:\n",
        "- Se evalúa el modelo en test, nunca en train.\n",
        "- Se comparan resultados con otros modelos.\n",
        "- Se decide si el rendimiento es suficiente o si hay que volver a pasos anteriores (features, modelo o métrica).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rx7Y--1T_nRi"
      },
      "outputs": [],
      "source": [
        "# Crear y entrenar el modelo\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyGe-6se7FFb"
      },
      "source": [
        "## Paso 10: Hiperparametrización/Regularización"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQqZFkSH_s7r"
      },
      "source": [
        "### Elegir hiperparámetros\n",
        "\n",
        "Como ya sabes, cada dataset es de su padre y de su madre, y por tanto es imposible determinar el modelo con sus hiperparámetros que mejor se ajusten a los datos. Por tanto, tendremos que probar varias combinaciones. Por suerte sklearn tiene una función llamada GridSearchCV que permite probar varias combinaciones de una manera automatizada.\n",
        "\n",
        "Empieza iterando unos pocos hiperparámetros y luego ve subiendo, según los resultados de esa ejecución.\n",
        "\n",
        "En este apartado veremos posibles hiperparámetros a emplear en los algoritmos más utilizados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "TzBKbiKfonL_",
        "outputId": "175a6350-bb33-4243-a5ef-8dfc7a9fa2e2"
      },
      "outputs": [],
      "source": [
        "# REGRESION LOGISTICA\n",
        "grid_logreg = {\n",
        "                     \"penalty\": [\"l1\",\"l2\"], # Regularizaciones L1 y L2.\n",
        "                     \"C\": [0.1, 0.5, 1.0, 5.0], # Cuanta regularizacion queremos\n",
        "\n",
        "                     \"max_iter\": [50,100,500],  # Iteraciones del Gradient Descent. No suele impactar mucho\n",
        "                                                # pero en ocasiones aparecen warnings diciendo que se aumente\n",
        "\n",
        "                     \"solver\": [\"liblinear\"]  # Suele ser el más rápido\n",
        "                    }\n",
        "\n",
        "\n",
        "# ARBOL DE DECISION\n",
        "grid_arbol = {\n",
        "    \"max_depth\": list(range(1, 10)),\n",
        "    \"min_samples_split\": [2, 5, 10],\n",
        "    \"min_samples_leaf\": [1, 2, 5]\n",
        "}\n",
        "\n",
        "# RANDOM FOREST\n",
        "grid_random_forest = {\"n_estimators\": [120], # El Random Forest no suele empeorar por exceso de\n",
        "                                             # estimadores. A partir de cierto numero no merece la pena\n",
        "                                             # perder el tiempo ya que no mejora mucho más la precisión.\n",
        "                                             # Entre 100 y 200 es una buena cifra\n",
        "\n",
        "\n",
        "                     \"max_depth\": [3,4,5,6,10,15,17], # No le afecta tanto el overfitting como al decissiontree.\n",
        "                                                      # Podemos probar mayores profundidades\n",
        "\n",
        "                     \"max_features\": [\"sqrt\", 3, 4] # Numero de features que utiliza en cada split.\n",
        "                                                    # cuanto más bajo, mejor generalizará y menos overfitting.\n",
        "\n",
        "                     }\n",
        "\n",
        "\n",
        "# GRADIENT BOOSTING\n",
        "grid_gradient_boosting = {\"loss\": [\"log_loss\"],\n",
        "                          \"learning_rate\": [0.05, 0.1, 0.2, 0.4, 0.5],  # Cuanto más alto, mas aporta cada nuevo arbol\n",
        "\n",
        "                          \"n_estimators\": [20,50,100,200], # Cuidado con poner muchos estiamdores ya que vamos a\n",
        "                                                           # sobreajustar el modelo\n",
        "\n",
        "                          \"max_depth\": [1,2,3,4,5], # No es necesario poner una profundiad muy alta. Cada nuevo\n",
        "                                                    # arbol va corrigiendo el error de los anteriores.\n",
        "\n",
        "\n",
        "                          \"max_features\": [\"sqrt\", 3, 4], # Igual que en el random forest\n",
        "                          }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxXGrv8dsEHu"
      },
      "source": [
        "Ejemplo de uso completo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRdoxv5QsGD9"
      },
      "outputs": [],
      "source": [
        "# Modelo base\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Grid de hiperparámetros\n",
        "param_grid = {\n",
        "    \"C\": [0.01, 0.1, 1, 10],\n",
        "    \"penalty\": [\"l2\"]\n",
        "}\n",
        "\n",
        "# GridSearch\n",
        "grid = GridSearchCV(\n",
        "    estimator=model,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring=\"accuracy\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Entrenamiento SOLO con train\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Mejor modelo encontrado\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "print(\"Mejores hiperparámetros:\", grid.best_params_)\n",
        "print(\"Mejor score CV:\", grid.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ythO-krz_3bI"
      },
      "source": [
        "### Regularización: controlando el sobreajuste\n",
        "La regularización sirve para evitar que el modelo se adapte demasiado a los datos de entrenamiento (overfitting), penalizando coeficientes grandes o complejidad excesiva.\n",
        "- L1 (Lasso): penaliza la suma de valores absolutos de los coeficientes. Puede eliminar features innecesarias.\n",
        "- L2 (Ridge): penaliza la suma de los cuadrados de los coeficientes. Reduce los coeficientes grandes, pero no los anula.\n",
        "- ElasticNet: combina L1 y L2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3enKYohABG2"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
        "\n",
        "# Ridge (L2)\n",
        "ridge = Ridge(alpha=1.0)\n",
        "ridge.fit(X_train, y_train)\n",
        "print(\"Ridge RMSE:\", round(mean_squared_error(y_test, ridge.predict(X_test), squared=False), 2))\n",
        "\n",
        "# Lasso (L1)\n",
        "lasso = Lasso(alpha=0.1)\n",
        "lasso.fit(X_train, y_train)\n",
        "print(\"Lasso RMSE:\", round(mean_squared_error(y_test, lasso.predict(X_test), squared=False), 2))\n",
        "\n",
        "# ElasticNet (L1 + L2)\n",
        "en = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
        "en.fit(X_train, y_train)\n",
        "print(\"ElasticNet RMSE:\", round(mean_squared_error(y_test, en.predict(X_test), squared=False), 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXklaOrTAJ78"
      },
      "source": [
        "\n",
        "- alpha controla la fuerza de la penalización: más alto → más regularización.\n",
        "- Ridge suaviza todos los coeficientes; Lasso puede dejar algunos en cero.\n",
        "- ElasticNet es útil si queremos un equilibrio entre ambos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwlSFAm_7Tgz"
      },
      "source": [
        "## Paso 11: Evaluación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pHnJHlvAO1_"
      },
      "source": [
        "Una vez elegidas las métricas y evaluados varios modelos, ya tendríamos los resultados del modelo. Sólo queda interpretarlos. Por desgracia este punto va a depender mucho del negocio.\n",
        "\n",
        "1. Evaluar los resutlados en test: tenemos que comprobar que los resultados no difieren mucho del entrenamiento. Si son inferiores es debido a que el modelo está sobreentrenado (ver abajo para solucionarlo). Si son superiores es raro, probablemente porque no tengamos muchos datos. Habría que intentar coseguir más observaciones.\n",
        "2. Evaluar los resultados respecto a las necesidades de negocio\n",
        "3. Almacenar todas las muestras utilizadas en el entrenamiento, así como los scripts y el propio modelo entrenado. La puesta en producción no es el objetivo a cobrir en este notebook.\n",
        "4. Elegir y evaluar la/las métrica/s con las que presentaremos los resultados del modelo. No tenemos por qué elegir la misma métrica utilizada en la búsqueda del mejor modelo, ya que no siempre es la más entendible: MSE o AUC.\n",
        "\n",
        "**¿Qué hacer si tenemos overfitting?**\n",
        "\n",
        "Posibles opciones para reducir el overfitting\n",
        "\n",
        "- Cross validation: si hemos hecho bien el paso anterior, el cross validation es la mejor técnica para evitar el overfitting.\n",
        "- Entrenar con más datos: no quitar datos a test, sino intentar conseguir más datos.\n",
        "- Eliminar features: PCA o un algoritmo de feature selection podría ser una buena solución\n",
        "- Max depth: reducirlo si estamos con árboles\n",
        "- Regularización: aumentarla en los modelos que permitan regulaización como regresiones lineales y SVM\n",
        "- Ensembles: sobretodo los algoritmos de bagging como el random forest no suelen sobreajustarse tanto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV0FH44LAuBT"
      },
      "source": [
        "Ejemplo de regresión:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VNS7rtsAOrl"
      },
      "outputs": [],
      "source": [
        "# Predicciones\n",
        "y_train_pred = model.predict(X_train)\n",
        "y_test_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluación\n",
        "print(\"Train RMSE:\", round(mean_squared_error(y_train, y_train_pred, squared=False), 2))\n",
        "print(\"Test RMSE:\", round(mean_squared_error(y_test, y_test_pred, squared=False), 2))\n",
        "print(\"Train R2:\", round(r2_score(y_train, y_train_pred), 2))\n",
        "print(\"Test R2:\", round(r2_score(y_test, y_test_pred), 2))\n",
        "\n",
        "# Cross-validation\n",
        "cv_scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
        "print(\"Cross-validated R2:\", round(cv_scores.mean(), 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSeBcSybAz9g"
      },
      "source": [
        "Si Train RMSE mucho menor que Test RMSE → overfitting\n",
        "\n",
        "Si Train R2 mucho mayor que Test R2 → overfitting\n",
        "\n",
        "Cross-validation nos da una idea más robusta del desempeño general del modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbdNxEGEAvxB"
      },
      "source": [
        "Ejemplo de clasificación:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQaXBo5SAxZM"
      },
      "outputs": [],
      "source": [
        "# Predicciones\n",
        "y_train_pred = model.predict(X_train)\n",
        "y_test_pred = model.predict(X_test)\n",
        "\n",
        "# Métricas\n",
        "print(\"Train Accuracy:\", round(accuracy_score(y_train, y_train_pred), 3))\n",
        "print(\"Test Accuracy:\", round(accuracy_score(y_test, y_test_pred), 3))\n",
        "print(\"Test Precision:\", round(precision_score(y_test, y_test_pred), 3))\n",
        "print(\"Test Recall:\", round(recall_score(y_test, y_test_pred), 3))\n",
        "print(\"Test F1:\", round(f1_score(y_test, y_test_pred), 3))\n",
        "\n",
        "# Matriz de confusión\n",
        "c_matrix = confusion_matrix(y_test, y_test_pred)\n",
        "sns.heatmap(c_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Cross-validation para detectar overfitting\n",
        "cv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
        "print(\"Cross-validated Accuracy:\", round(cv_scores.mean(), 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfnpfsCqA61M"
      },
      "source": [
        "Interpretación:\n",
        "- Si Train Accuracy mucho mayor que Test Accuracy → overfitting\n",
        "- Precision y Recall ayudan a entender errores según la clase:\n",
        "  - Recall alto → pocos falsos negativos\n",
        "  - Precision alto → pocos falsos positivos\n",
        "- La matriz de confusión muestra en detalle dónde el modelo se confunde.\n",
        "- Cross-validation da una medida más estable y general del desempeño del modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRuzg6YE7UD8"
      },
      "source": [
        "## Paso 12: Conclusión"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g37UHxAOBfnw"
      },
      "source": [
        "Al finalizar un proyecto de ML, las conclusiones deben centrarse en estos puntos:\n",
        "1. Rendimiento general del modelo\n",
        "- Qué tan bien predice en comparación con los datos de test.\n",
        "- Métricas clave (accuracy, F1, R², MSE, etc.) y su interpretación concreta: “El modelo acierta un 82% de los casos y su F1-score indica un buen equilibrio entre precisión y recall”.\n",
        "2. Errores y limitaciones\n",
        "- Identificar patrones de error: qué casos falla más y por qué.\n",
        "- Posibles sesgos: datos desbalanceados, features que afectan demasiado o poco.\n",
        "- Riesgos de overfitting o underfitting.\n",
        "3. Importancia de variables / interpretabilidad\n",
        "- Qué features fueron más determinantes en las predicciones.\n",
        "- Cómo esas variables se relacionan con el resultado esperado.\n",
        "4. Aplicabilidad para el negocio o contexto\n",
        "- Cómo los resultados ayudan a la toma de decisiones.\n",
        "- Limitaciones prácticas: casos donde no conviene usar el modelo o donde los errores pueden ser críticos.\n",
        "5. Recomendaciones futuras\n",
        "- Mejoras posibles: más datos, refinamiento de features, ajuste de hiperparámetros, pruebas con otros modelos.\n",
        "- Estrategias para mantener el modelo actualizado y confiable en producción.\n",
        "\n",
        "**Ejemplo de redacción para presentar resultados:**\n",
        "\n",
        "*“El modelo de clasificación logra un 82% de accuracy y un F1-score de 0.77, lo que indica un buen equilibrio entre precisión y recall. Detecta correctamente la mayoría de los casos positivos, aunque confunde principalmente X con Y. Las variables más relevantes fueron A, B y C, lo que concuerda con el conocimiento del negocio. Se recomienda seguir recopilando datos y evaluar regularmente el rendimiento para evitar sesgos futuros.”*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
